{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8242790,"sourceType":"datasetVersion","datasetId":4889854}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\nimport torch\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import EarlyStoppingCallback\ndata = pd.read_csv('/kaggle/input/binary-class/binary_class.csv')\nmodel_name = \"cointegrated/rubert-tiny2\"\ndevice = 'cuda' if torch.cuda.is_available else 'cpu'\n\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-27T07:58:45.777139Z","iopub.execute_input":"2024-04-27T07:58:45.777484Z","iopub.status.idle":"2024-04-27T07:58:48.187560Z","shell.execute_reply.started":"2024-04-27T07:58:45.777458Z","shell.execute_reply":"2024-04-27T07:58:48.186551Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"X = list(data[\"Текст сообщения\"])\ny = list(data[\"Разметка\"])","metadata":{"execution":{"iopub.status.busy":"2024-04-27T07:51:28.413527Z","iopub.execute_input":"2024-04-27T07:51:28.413874Z","iopub.status.idle":"2024-04-27T07:51:28.419657Z","shell.execute_reply.started":"2024-04-27T07:51:28.413848Z","shell.execute_reply":"2024-04-27T07:51:28.418452Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True)\nX_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=128)\nX_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=128)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T07:51:33.213959Z","iopub.execute_input":"2024-04-27T07:51:33.214951Z","iopub.status.idle":"2024-04-27T07:51:33.776547Z","shell.execute_reply.started":"2024-04-27T07:51:33.214917Z","shell.execute_reply":"2024-04-27T07:51:33.775751Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.labels:\n            item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n\ntrain_dataset = Dataset(X_train_tokenized, y_train)\nval_dataset = Dataset(X_val_tokenized, y_val)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T07:58:49.202799Z","iopub.execute_input":"2024-04-27T07:58:49.203542Z","iopub.status.idle":"2024-04-27T07:58:49.211327Z","shell.execute_reply.started":"2024-04-27T07:58:49.203509Z","shell.execute_reply":"2024-04-27T07:58:49.210322Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(p):\n    pred, labels = p\n    pred = np.argmax(pred, axis=1)\n\n    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n    recall = recall_score(y_true=labels, y_pred=pred, average = 'macro')\n    precision = precision_score(y_true=labels, y_pred=pred, average = 'macro')\n    f1 = f1_score(y_true=labels, y_pred=pred, average = 'macro')\n\n    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n\n# Define Trainer\nargs = TrainingArguments(\n    output_dir=\"/output\",\n    evaluation_strategy=\"steps\",\n    eval_steps=50,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=128,\n    num_train_epochs=30,\n    weight_decay=0.05,\n    learning_rate = 1e-5,\n    warmup_ratio=0.2,\n    lr_scheduler_type = 'cosine',\n    seed=20222022,\n    load_best_model_at_end=True,\n)\ntrainer = Trainer(\n    model=model,    \n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n#     callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T07:58:49.547805Z","iopub.execute_input":"2024-04-27T07:58:49.548456Z","iopub.status.idle":"2024-04-27T08:00:10.670712Z","shell.execute_reply.started":"2024-04-27T07:58:49.548416Z","shell.execute_reply":"2024-04-27T08:00:10.669650Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3030' max='3030' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3030/3030 01:20, Epoch 30/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>No log</td>\n      <td>0.691619</td>\n      <td>0.538653</td>\n      <td>0.518103</td>\n      <td>0.516819</td>\n      <td>0.514181</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>No log</td>\n      <td>0.687403</td>\n      <td>0.600998</td>\n      <td>0.588164</td>\n      <td>0.559166</td>\n      <td>0.539054</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>No log</td>\n      <td>0.682973</td>\n      <td>0.603491</td>\n      <td>0.606708</td>\n      <td>0.550839</td>\n      <td>0.508885</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>No log</td>\n      <td>0.673474</td>\n      <td>0.613466</td>\n      <td>0.749213</td>\n      <td>0.548284</td>\n      <td>0.467400</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>No log</td>\n      <td>0.664363</td>\n      <td>0.635910</td>\n      <td>0.758370</td>\n      <td>0.575350</td>\n      <td>0.516947</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>No log</td>\n      <td>0.651356</td>\n      <td>0.648379</td>\n      <td>0.769292</td>\n      <td>0.589969</td>\n      <td>0.540933</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>No log</td>\n      <td>0.636103</td>\n      <td>0.655860</td>\n      <td>0.740504</td>\n      <td>0.601742</td>\n      <td>0.565336</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>No log</td>\n      <td>0.618500</td>\n      <td>0.680798</td>\n      <td>0.693023</td>\n      <td>0.646733</td>\n      <td>0.642662</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>No log</td>\n      <td>0.602078</td>\n      <td>0.710723</td>\n      <td>0.712282</td>\n      <td>0.687821</td>\n      <td>0.690290</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.655400</td>\n      <td>0.582547</td>\n      <td>0.705736</td>\n      <td>0.705654</td>\n      <td>0.683473</td>\n      <td>0.685803</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.655400</td>\n      <td>0.569342</td>\n      <td>0.710723</td>\n      <td>0.708995</td>\n      <td>0.690821</td>\n      <td>0.693522</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.655400</td>\n      <td>0.558805</td>\n      <td>0.718204</td>\n      <td>0.711846</td>\n      <td>0.710844</td>\n      <td>0.711303</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.655400</td>\n      <td>0.551745</td>\n      <td>0.728180</td>\n      <td>0.725950</td>\n      <td>0.711289</td>\n      <td>0.714427</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.655400</td>\n      <td>0.544385</td>\n      <td>0.723192</td>\n      <td>0.717122</td>\n      <td>0.717442</td>\n      <td>0.717277</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.655400</td>\n      <td>0.541749</td>\n      <td>0.720698</td>\n      <td>0.714392</td>\n      <td>0.713018</td>\n      <td>0.713630</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.655400</td>\n      <td>0.540309</td>\n      <td>0.720698</td>\n      <td>0.715285</td>\n      <td>0.717518</td>\n      <td>0.716106</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.655400</td>\n      <td>0.541248</td>\n      <td>0.730673</td>\n      <td>0.726305</td>\n      <td>0.717213</td>\n      <td>0.719798</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.655400</td>\n      <td>0.539663</td>\n      <td>0.718204</td>\n      <td>0.712386</td>\n      <td>0.713844</td>\n      <td>0.713000</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.655400</td>\n      <td>0.536771</td>\n      <td>0.725686</td>\n      <td>0.719729</td>\n      <td>0.720366</td>\n      <td>0.720029</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.505400</td>\n      <td>0.540995</td>\n      <td>0.743142</td>\n      <td>0.737993</td>\n      <td>0.733333</td>\n      <td>0.735069</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.505400</td>\n      <td>0.541845</td>\n      <td>0.735661</td>\n      <td>0.729775</td>\n      <td>0.729062</td>\n      <td>0.729399</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.505400</td>\n      <td>0.546225</td>\n      <td>0.718204</td>\n      <td>0.712908</td>\n      <td>0.715345</td>\n      <td>0.713754</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.505400</td>\n      <td>0.548346</td>\n      <td>0.745636</td>\n      <td>0.740476</td>\n      <td>0.736257</td>\n      <td>0.737875</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.505400</td>\n      <td>0.549778</td>\n      <td>0.743142</td>\n      <td>0.737993</td>\n      <td>0.733333</td>\n      <td>0.735069</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.505400</td>\n      <td>0.552825</td>\n      <td>0.728180</td>\n      <td>0.722129</td>\n      <td>0.721790</td>\n      <td>0.721955</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.505400</td>\n      <td>0.552833</td>\n      <td>0.740648</td>\n      <td>0.735114</td>\n      <td>0.731909</td>\n      <td>0.733200</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.505400</td>\n      <td>0.559036</td>\n      <td>0.735661</td>\n      <td>0.729812</td>\n      <td>0.729812</td>\n      <td>0.729812</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.505400</td>\n      <td>0.562765</td>\n      <td>0.735661</td>\n      <td>0.729775</td>\n      <td>0.729062</td>\n      <td>0.729399</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.505400</td>\n      <td>0.568452</td>\n      <td>0.738155</td>\n      <td>0.732334</td>\n      <td>0.731236</td>\n      <td>0.731742</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.395000</td>\n      <td>0.571718</td>\n      <td>0.738155</td>\n      <td>0.733031</td>\n      <td>0.727485</td>\n      <td>0.729432</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.395000</td>\n      <td>0.572189</td>\n      <td>0.740648</td>\n      <td>0.734910</td>\n      <td>0.734910</td>\n      <td>0.734910</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.395000</td>\n      <td>0.582073</td>\n      <td>0.738155</td>\n      <td>0.734502</td>\n      <td>0.724485</td>\n      <td>0.727301</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.395000</td>\n      <td>0.575407</td>\n      <td>0.748130</td>\n      <td>0.742656</td>\n      <td>0.740681</td>\n      <td>0.741546</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.395000</td>\n      <td>0.582474</td>\n      <td>0.743142</td>\n      <td>0.737625</td>\n      <td>0.734833</td>\n      <td>0.735990</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.395000</td>\n      <td>0.584741</td>\n      <td>0.740648</td>\n      <td>0.735114</td>\n      <td>0.731909</td>\n      <td>0.733200</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.395000</td>\n      <td>0.585748</td>\n      <td>0.743142</td>\n      <td>0.737446</td>\n      <td>0.737084</td>\n      <td>0.737260</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.395000</td>\n      <td>0.589645</td>\n      <td>0.738155</td>\n      <td>0.732334</td>\n      <td>0.731236</td>\n      <td>0.731742</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.395000</td>\n      <td>0.593464</td>\n      <td>0.743142</td>\n      <td>0.737446</td>\n      <td>0.737084</td>\n      <td>0.737260</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.395000</td>\n      <td>0.598655</td>\n      <td>0.733167</td>\n      <td>0.727427</td>\n      <td>0.723888</td>\n      <td>0.725267</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.324300</td>\n      <td>0.598938</td>\n      <td>0.740648</td>\n      <td>0.734910</td>\n      <td>0.734910</td>\n      <td>0.734910</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.324300</td>\n      <td>0.602036</td>\n      <td>0.740648</td>\n      <td>0.734888</td>\n      <td>0.734160</td>\n      <td>0.734505</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.324300</td>\n      <td>0.603137</td>\n      <td>0.738155</td>\n      <td>0.732340</td>\n      <td>0.731986</td>\n      <td>0.732158</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.324300</td>\n      <td>0.605953</td>\n      <td>0.738155</td>\n      <td>0.732340</td>\n      <td>0.731986</td>\n      <td>0.732158</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.324300</td>\n      <td>0.607033</td>\n      <td>0.735661</td>\n      <td>0.729841</td>\n      <td>0.727562</td>\n      <td>0.728528</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.324300</td>\n      <td>0.609685</td>\n      <td>0.730673</td>\n      <td>0.724909</td>\n      <td>0.720964</td>\n      <td>0.722456</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.324300</td>\n      <td>0.612430</td>\n      <td>0.740648</td>\n      <td>0.734915</td>\n      <td>0.733410</td>\n      <td>0.734085</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.324300</td>\n      <td>0.615968</td>\n      <td>0.735661</td>\n      <td>0.729841</td>\n      <td>0.727562</td>\n      <td>0.728528</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.324300</td>\n      <td>0.616282</td>\n      <td>0.735661</td>\n      <td>0.729812</td>\n      <td>0.729812</td>\n      <td>0.729812</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.324300</td>\n      <td>0.618288</td>\n      <td>0.738155</td>\n      <td>0.732334</td>\n      <td>0.731236</td>\n      <td>0.731742</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.279000</td>\n      <td>0.619512</td>\n      <td>0.740648</td>\n      <td>0.734888</td>\n      <td>0.734160</td>\n      <td>0.734505</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>0.279000</td>\n      <td>0.620938</td>\n      <td>0.735661</td>\n      <td>0.729784</td>\n      <td>0.728312</td>\n      <td>0.728971</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.279000</td>\n      <td>0.622069</td>\n      <td>0.735661</td>\n      <td>0.729945</td>\n      <td>0.726812</td>\n      <td>0.728070</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>0.279000</td>\n      <td>0.621581</td>\n      <td>0.738155</td>\n      <td>0.732376</td>\n      <td>0.730486</td>\n      <td>0.731310</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.279000</td>\n      <td>0.621705</td>\n      <td>0.738155</td>\n      <td>0.732376</td>\n      <td>0.730486</td>\n      <td>0.731310</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>0.279000</td>\n      <td>0.622081</td>\n      <td>0.738155</td>\n      <td>0.732376</td>\n      <td>0.730486</td>\n      <td>0.731310</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.279000</td>\n      <td>0.622515</td>\n      <td>0.738155</td>\n      <td>0.732376</td>\n      <td>0.730486</td>\n      <td>0.731310</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>0.279000</td>\n      <td>0.622575</td>\n      <td>0.738155</td>\n      <td>0.732376</td>\n      <td>0.730486</td>\n      <td>0.731310</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.279000</td>\n      <td>0.622578</td>\n      <td>0.738155</td>\n      <td>0.732376</td>\n      <td>0.730486</td>\n      <td>0.731310</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>0.279000</td>\n      <td>0.622658</td>\n      <td>0.738155</td>\n      <td>0.732376</td>\n      <td>0.730486</td>\n      <td>0.731310</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.273400</td>\n      <td>0.622681</td>\n      <td>0.738155</td>\n      <td>0.732376</td>\n      <td>0.730486</td>\n      <td>0.731310</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3030, training_loss=0.4042874342537556, metrics={'train_runtime': 80.4464, 'train_samples_per_second': 597.416, 'train_steps_per_second': 37.665, 'total_flos': 88601096816640.0, 'train_loss': 0.4042874342537556, 'epoch': 30.0})"},"metadata":{}}]},{"cell_type":"code","source":"# model.save_pretrained('bin(importance_classification)')","metadata":{"execution":{"iopub.status.busy":"2024-04-27T08:01:07.846325Z","iopub.execute_input":"2024-04-27T08:01:07.847454Z","iopub.status.idle":"2024-04-27T08:01:08.130557Z","shell.execute_reply.started":"2024-04-27T08:01:07.847369Z","shell.execute_reply":"2024-04-27T08:01:08.129461Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"X_test = tokenizer(['угабуга'], padding=True, truncation=True, max_length=128)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T08:08:32.514446Z","iopub.execute_input":"2024-04-27T08:08:32.515243Z","iopub.status.idle":"2024-04-27T08:08:32.520616Z","shell.execute_reply.started":"2024-04-27T08:08:32.515211Z","shell.execute_reply":"2024-04-27T08:08:32.519631Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"test_dataset = Dataset(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T08:08:33.079216Z","iopub.execute_input":"2024-04-27T08:08:33.080230Z","iopub.status.idle":"2024-04-27T08:08:33.085535Z","shell.execute_reply.started":"2024-04-27T08:08:33.080191Z","shell.execute_reply":"2024-04-27T08:08:33.084295Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"test_trainer = Trainer(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T08:08:33.452052Z","iopub.execute_input":"2024-04-27T08:08:33.452642Z","iopub.status.idle":"2024-04-27T08:08:33.496231Z","shell.execute_reply.started":"2024-04-27T08:08:33.452589Z","shell.execute_reply":"2024-04-27T08:08:33.495184Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"raw_pred, _, _ = test_trainer.predict(test_dataset) # Классификация запроса\ny_pred = torch.argmax(torch.softmax(torch.tensor(raw_pred), dim=-1), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T08:08:34.094988Z","iopub.execute_input":"2024-04-27T08:08:34.095329Z","iopub.status.idle":"2024-04-27T08:08:34.114905Z","shell.execute_reply.started":"2024-04-27T08:08:34.095301Z","shell.execute_reply":"2024-04-27T08:08:34.113841Z"},"trusted":true},"execution_count":45,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained('bin_model(importance_classification)')\ntokenizer.save_pretrained('bin_tokenizer(importance_classification)')","metadata":{"execution":{"iopub.status.busy":"2024-04-27T08:18:56.290539Z","iopub.execute_input":"2024-04-27T08:18:56.291529Z","iopub.status.idle":"2024-04-27T08:18:56.657180Z","shell.execute_reply.started":"2024-04-27T08:18:56.291491Z","shell.execute_reply":"2024-04-27T08:18:56.656067Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"('bin_tokenizer(importance_classification)/tokenizer_config.json',\n 'bin_tokenizer(importance_classification)/special_tokens_map.json',\n 'bin_tokenizer(importance_classification)/vocab.txt',\n 'bin_tokenizer(importance_classification)/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport zipfile\ntokenizer_path = '/kaggle/working/bin_tokenizer(importance_classification)'\nmodel_path = '/kaggle/working/bin_model(importance_classification)'\n\narchive_name = \"bert_model_archive.zip\"\nwith zipfile.ZipFile(archive_name, \"w\") as archive:\n    # Add model files\n    archive.write(os.path.join(model_path, \"config.json\"), \"config.json\")\n    archive.write(os.path.join(model_path, \"model.safetensors\"), \"model.safetensors\")\n    # Add tokenizer files\n    archive.write(os.path.join(tokenizer_path, \"vocab.txt\"), \"vocab.txt\")\n    archive.write(os.path.join(tokenizer_path, \"special_tokens_map.json\"), \"special_tokens_map.json\")\n    archive.write(os.path.join(tokenizer_path, \"tokenizer_config.json\"), \"tokenizer_config.json\")","metadata":{"execution":{"iopub.status.busy":"2024-04-27T08:20:25.946169Z","iopub.execute_input":"2024-04-27T08:20:25.947013Z","iopub.status.idle":"2024-04-27T08:20:26.311778Z","shell.execute_reply.started":"2024-04-27T08:20:25.946976Z","shell.execute_reply":"2024-04-27T08:20:26.310548Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}